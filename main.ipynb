{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sep-eg/SubjectClassifier-usingYoutubeSubtitle/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOGJ83P6chC5"
      },
      "source": [
        "#AI BootCamp Project\n",
        "\n",
        "네이버 지식인을 이용하여 질문글을 작성하거나, 당근마켓을 통해 중고거래 글을 작성하려 할 때 등 내가 입력한 내용을 기반으로 자동으로 카테고리를 추천해주는 경험을 해본적이 있을 것이다. \n",
        "\n",
        "이러한 기능은 글을 작성하는 작성자가 특별히 신경쓰지 않더라도 내용을 검색해보고 싶은 검색자가 쉽게 찾을 수 있도록 카테고리 분류작업을 대신 해줄수 있다.\n",
        "\n",
        "하지만 만약 영상데이터라면 그 분류를 어떻게 진행할 수 있을까? 나는 이 고민을 유튜브 자막 데이터를 이용하여 해결해 보기로하였다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P_x93MffZSg"
      },
      "source": [
        "## 데이터 불러오기 및 전처리\n",
        "\n",
        "데이터는 kaggle에 있는 Math Lectures(https://www.kaggle.com/extralime/math-lectures) 를 활용하였다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJRyy8uy5jQM"
      },
      "source": [
        "### 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9Klo3HEedPz"
      },
      "source": [
        "# # 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA-T_ilrgCUx"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9Co1q4zRFUY"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGXMfDvEb2lZ"
      },
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/sec4project/'\n",
        "df = pd.read_csv(BASE_PATH+'raw_text.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dLQoe2_gNgW"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbcfdRCCgRnn"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JKMWeG_gXPZ"
      },
      "source": [
        "데이터는 860개의 영상의 subtitle text와 어떤 분야의 영상인지를 나타내는 label을 가지고 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tayh1dxf5pQG"
      },
      "source": [
        "### 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yda_YLQ4gjJJ"
      },
      "source": [
        "# 결측치확인 -> 없음\n",
        "df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpoKmO_qgsha"
      },
      "source": [
        "# text와 label 분리\n",
        "texts = df['text']\n",
        "label = df['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IORoPXrhVaX"
      },
      "source": [
        "# 중복데이터 확인\n",
        "texts.duplicated().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPprPqkqk5C4"
      },
      "source": [
        "중복된 데이터가 한 쌍 발견되었으므로 어떤것을 지워야할지 확인한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfHm2WGqhcIN"
      },
      "source": [
        "# 중복된 데이터의 인덱스 확인\n",
        "texts[texts.duplicated()==True].index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNntCHtSiLzw"
      },
      "source": [
        "# 중복된 데이터와 같은 데이터가 위치한 인덱스 확인\n",
        "sum(texts.drop_duplicates(keep='first').index) - sum(texts.drop_duplicates(keep='last').index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jcd8-SPDkHQ2"
      },
      "source": [
        "# 탐색한 인덱스에 위치한 데이터가 중복이 맞는지 확인\n",
        "texts[443] == texts[408]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHTWTmeykON1"
      },
      "source": [
        "# 두데이터 레이블 확인\n",
        "label[443], label[408]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgiLyuW2kqt9"
      },
      "source": [
        "texts[443]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3za3PKK3m8U"
      },
      "source": [
        "강의 내용이 Probability에 관한 내용으로 판단되므로 CS레이블로 분류된 408번 인덱스의 데이터를 제거한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T92b8EZe3mns"
      },
      "source": [
        "# 인덱스 443 제거\n",
        "texts.drop(index=443, inplace=True)\n",
        "label.drop(index=443, inplace=True)\n",
        "texts.shape, label.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hVJI2536W1u"
      },
      "source": [
        "텍스트 데이터 시작 부분에 같은내용이 반복되는 부분이 있으므로 삭제를 위해 데이터 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuOKIKfg5YMj"
      },
      "source": [
        "texts[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZw-pcoY6TMU"
      },
      "source": [
        "texts[3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmjwq6iq6p9n"
      },
      "source": [
        "The following content ... ocw.mit.edu가 반복적으로 들어가는 것을 확인 해당부분 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7hJGLqg6zvW"
      },
      "source": [
        "\"\"\"\n",
        "The following content ... ocw.mit.edu.\n",
        "로 중복되는 구간을 제거하는 함수\n",
        "\"\"\"\n",
        "\n",
        "def cut_Useless(text):\n",
        "\n",
        "    if (text.startswith(\"The following content\") == True\n",
        "        or text.startswith(\"The following\\ncontent\")==True): \n",
        "        #ocw.mit.edu. 를 기준으로 split하고 앞부분을 스킵해서 제거 \n",
        "        for idx, sentence in enumerate(text.split(\"ocw.mit.edu. \")):\n",
        "            if idx==0:\n",
        "                continue\n",
        "            return sentence\n",
        "\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6osV2m19u7e"
      },
      "source": [
        "texts = texts.apply(cut_Useless)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP9aMoj4Kg6C"
      },
      "source": [
        "본격적인 전처리를 진행하기전 대부분의 text가 \"\\n\"을 구분기호로 사용하고 있는것으로 보이지만 그렇지 않은경우 문자열을 분리를 할때 문제가 생길 수 있으므로 없는 text가 있는지 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toq_GawBJ0pY"
      },
      "source": [
        "texts_len = len(texts)\n",
        "for idx, text in enumerate(texts):\n",
        "    if \"\\n\" in text == False:\n",
        "        print(idx)\n",
        "        continue\n",
        "    texts_len -= 1\n",
        "\n",
        "if texts_len == 0:\n",
        "    print(\"\\\\n을 포함하지 않은 text는 없습니다.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlBkM6X1LOgD"
      },
      "source": [
        "texts[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxTEfWXJMnvT"
      },
      "source": [
        "---\n",
        "text를 확인해보면 자막데이터인 관계로 \"\\n\"은 문장의 끝과는 상관없는 것을 확인할 수 있다.\n",
        "\n",
        "1. 따라서 \"\\n\"은 공백으로 대체하고 추후 \".\"를 기준으로 split 진행\n",
        "\n",
        "2. 또한 문장을 나누는 기준으로 \".\"을 사용하기 때문에 \"?\", \"!\"또한 \".\"으로 대체한다.\n",
        "\n",
        "3. 영상의 주제에 영향을 끼치는건 구체적인 계산식 보다는 중요 키워드의 영향이 크다고 생각되기 때문에 영문자를 제외한 텍스트는 제거한다.\n",
        "\n",
        "4. 리소스를 줄이기위해 주제를 판단하는데 미치는 영향이 적은 불용어는 제거한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHApGW5JH9_w"
      },
      "source": [
        "\"\"\"\n",
        "특수문자 대체, 불용어 삭제 등 전반적인 전처리 함수\n",
        "\"\"\"\n",
        "\n",
        "def preprocessing(text, remove_stopwords=True):\n",
        "\n",
        "    text=text.replace(\"\\n\", \" \").replace(\"!\", \".\").replace(\"?\", \".\")\n",
        "    \n",
        "    text=re.sub(\"[^a-zA-Z.:]\", \" \", text)\n",
        "    # \":\"은 말하는 화자를 구분하는 의미로 사용되고 있으므로 의미를 살리기위해 제거하지 않는다.\n",
        "    \n",
        "    words=text.lower().split()\n",
        "    \n",
        "    if remove_stopwords:\n",
        "        # 불용어 제거\n",
        "        \n",
        "        # 영어 불용어 불러오기\n",
        "        stops=set(stopwords.words(\"english\"))\n",
        "\n",
        "        # 주로 수학, CS에 관한 내용이므로 변수등으로 많이 사용될 수 있는 알파벳 모음을 불용어에 추가\n",
        "        stops.update(['a','b','c','d','e',\n",
        "                      'f','g','h','i','j',\n",
        "                      'k','l','m','n','o',\n",
        "                      'p','q','r','s','t',\n",
        "                      'u','v','w','x','y','z'])\n",
        "        \n",
        "        # 불용어가 아닌 단어로 이뤄진 새로운 리스트 생성\n",
        "        words=[w for w in words if not w in stops]\n",
        "        # 5. 단어 리스트를 공백을 넣어서 하나의 글로 합친다.\n",
        "        text=' '.join(words)\n",
        "    \n",
        "    # 불용어를 제거하지 않을 때\n",
        "    else:\n",
        "        text=' '.join(words)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4lvSdsAQTfw"
      },
      "source": [
        "texts = texts.apply(preprocessing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNBLmmuNYSh3"
      },
      "source": [
        "texts[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA5u297RTbpM"
      },
      "source": [
        "###데이터 분리\n",
        "본격적으로 모델을 만들기 전 데이터를 train set과 test set으로 나누겠습니다.\n",
        "\n",
        "프로젝트의 재현성을 위해 이후 진행할 모든 작업에서 random_state는 11로 지정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEgma881R_Nm"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH72wa1RTyqQ"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(texts, label,\n",
        "                                                    train_size=0.8, stratify=label,\n",
        "                                                    random_state=11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpS6g0o0UFyS"
      },
      "source": [
        "X_train, X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga_N2vKUiXLf"
      },
      "source": [
        "### 레이블 변경\n",
        "현재의 문자형태로 되어있는 레이블은 추후 모델을 학습시킬때 장애가 될수있으므로 숫자형 데이터로 매칭하여 변환시킨다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89_NtwYFinEJ"
      },
      "source": [
        "label_name = y_train.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1tY6NUJiu-K"
      },
      "source": [
        "for idx, tmp_label in enumerate(label_name):\n",
        "    y_train.replace(tmp_label, idx, inplace=True)\n",
        "    y_test.replace(tmp_label, idx, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9mSY7InjW7O"
      },
      "source": [
        "y_train.unique(), y_test.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8ESacZAVSdm"
      },
      "source": [
        "## 모델링"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixACOWM5OrA-"
      },
      "source": [
        "### LSTM을 이용한 간단한모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClTCHPqvUmP7"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYkmZwrAXOW9"
      },
      "source": [
        "tokenizer = Tokenizer() # 리소스 한계로인해 단어개수는 15000개로 제한한다.\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9dwR7tXBfyT"
      },
      "source": [
        "# train_sequences중 가장 긴 sequence의 길이를 max_len으로 지정\n",
        "max_len = 0\n",
        "for sequen in train_sequences:\n",
        "    if max_len < len(sequen):\n",
        "        max_len = len(sequen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfHIG6BKapyg"
      },
      "source": [
        "# embedding을 위해 token화한 단어들의 개수를 vocab_size로 저장\n",
        "word_to_index = tokenizer.word_index\n",
        "vocab_size = len(word_to_index) + 1\n",
        "embedding_size = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzHPayRhe-1D"
      },
      "source": [
        "# sequence들을 패딩하는 과정, 모든 sequence의 길이를 맞춰준다.\n",
        "train_sequences = pad_sequences(train_sequences, maxlen=max_len)\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=max_len)\n",
        "\n",
        "# 11개의 노드를 통해 분류되기 때문에 label을 범주형으로 변환\n",
        "y_train_cate = to_categorical(y_train, num_classes=11)\n",
        "y_test_cate = to_categorical(y_test, num_classes=11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wajc4YoaalP4"
      },
      "source": [
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization, Flatten, concatenate, InputLayer\n",
        "from tensorflow.keras.models import Sequential, Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcQHQaxGgBPP"
      },
      "source": [
        "tf.random.set_seed(11)\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_size))\n",
        "model.add(LSTM(128))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(11, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
        "history1 = model.fit(train_sequences, y_train_cate, epochs=6, batch_size=32, validation_data=(test_sequences, y_test_cate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6slCiWUM5J8"
      },
      "source": [
        "acc = history1.history['accuracy']\n",
        "val_acc = history1.history['val_accuracy']\n",
        "\n",
        "sns.lineplot(x=range(len(acc)), y=acc)\n",
        "sns.lineplot(x=range(len(acc)), y=val_acc)\n",
        "plt.title('LSTM_model accuracy')\n",
        "plt.legend(['acc', 'val_acc']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEPQLg3T1-y_"
      },
      "source": [
        "LSTM을 이용해 간단한 모델을 구현해 보았을때, train set에 대해서 5epoch만에 정확도가 1.0으로 높게 나타나는 모습을 보였다.\n",
        "\n",
        "하지만 이는 train set에 과적합된 결과로 test set에 대해서는 정확도가 전혀 개선되지 않는 모습을 보였는데 이는 강의자막 데이터라는 특성상\n",
        "\n",
        "강사, 혹은 세부주제에 따라 내용의 흐름이 크게 달라질 수 있기 때문에 LSTM의 순차적 처리방식이 맞지 않은것으로 보았다.\n",
        "\n",
        "이에 따라 모든 문서의 단어를 확인하여 중요 단어를 판단하는 TF-IDF를 이용한 분류를 진행해 보기로 하였다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6T611k0Lcz5"
      },
      "source": [
        "# 학습된 모델 저장\n",
        "# model.save(BASE_PATH+\"LSTM_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BztkdbkZOxwf"
      },
      "source": [
        "### TF-IDF과정을 거친 데이터를 이용한 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYpxVm9E19PW"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C4z1c_S3Ejs"
      },
      "source": [
        "vector = TfidfVectorizer()\n",
        "train_tfidf = vector.fit_transform(X_train).toarray()\n",
        "test_tfidf = vector.transform(X_test).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW7k3PN2FOEO"
      },
      "source": [
        "max_features = train_tfidf.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2NXxihj5AIT"
      },
      "source": [
        "tf.random.set_seed(11)\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(InputLayer(input_shape=(max_features)))\n",
        "model2.add(Dense(256, activation='relu'))\n",
        "model2.add(Dense(128, activation='relu'))\n",
        "model2.add(Dense(11, activation='softmax'))\n",
        "\n",
        "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
        "history2 = model2.fit(train_tfidf, y_train_cate, epochs=6, batch_size=8, validation_data=(test_tfidf, y_test_cate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL9MkAUeLoO3"
      },
      "source": [
        "acc = history2.history['accuracy']\n",
        "val_acc = history2.history['val_accuracy']\n",
        "\n",
        "sns.lineplot(x=range(len(acc)), y=acc)\n",
        "sns.lineplot(x=range(len(acc)), y=val_acc)\n",
        "plt.title('tfidf_fully_model accuracy')\n",
        "plt.legend(['acc', 'val_acc']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH9BiSliLo16"
      },
      "source": [
        "# 학습된 모델 저장\n",
        "# model2.save(BASE_PATH+\"tfidf_fully_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2luQj_rpNI7T"
      },
      "source": [
        "TF-IDF를 통해 얻은 데이터를 이용한 학습도 마찬가지의 결과였다. \n",
        "\n",
        "train set에 대한 정확도는 빠르게 1.0을 향해 수렴해갔지만,\n",
        "\n",
        "test set에 대한 정확도는 여전히 거듭되는 학습에도 개선되지 못했다.\n",
        "\n",
        "(그래프는 6번의 epoch만을 나타냈지만 epoch수를 높여도 마찬가지였다.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hydtjUiTPPuM"
      },
      "source": [
        "### 새로운 접근의 필요(BERT)\n",
        "이러한 결과가 나타난 다양한 이유가 있을 수 있겠지만, 근본적인 데이터셋 부족이 가장 큰 이유로 보인다.\n",
        "\n",
        "당장 더 큰 데이터를 모으는 것은 어려우므로 이를 개선해보기 위하여 사전학습 모델 BERT에 fine-tuning을 적용하여 학습을 진행해 보았다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9km5kuSAk19p"
      },
      "source": [
        "# !pip install -q -U tensorflow-text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1c_34_fOknV"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMuYpUCElecn"
      },
      "source": [
        "bert_model_name = 'experts_wiki_books' \n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_yKOHAekzU0"
      },
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTFBJi4Ak_wO"
      },
      "source": [
        "def build_classifier_model():\n",
        "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "    encoder_inputs = preprocessing_layer(text_input)\n",
        "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=False, name='BERT_encoder')\n",
        "    outputs = encoder(encoder_inputs)\n",
        "    net = outputs['pooled_output']\n",
        "    net = Dropout(0.3)(net)\n",
        "    net = Dense(256, activation='relu')(net)\n",
        "    net = Dense(11, activation='softmax', name='classifier')(net)\n",
        "    return tf.keras.Model(text_input, net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JvEPAaVl5Dr"
      },
      "source": [
        "bert_model = build_classifier_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_mupWhxsocD"
      },
      "source": [
        "bert_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDiIMKk7mPaG"
      },
      "source": [
        "tf.random.set_seed(11)\n",
        "bert_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
        "# 모델에 preprocessing과정이 포함되어 있으므로 Tokenizer를 거치지않은 데이터를 활용하여 학습\n",
        "history3 = bert_model.fit(X_train, y_train_cate, epochs=50, batch_size=64, validation_data=(X_test, y_test_cate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5es8X-avMKf"
      },
      "source": [
        "acc = history3.history['accuracy']\n",
        "val_acc = history3.history['val_accuracy']\n",
        "\n",
        "sns.lineplot(x=range(len(acc)), y=acc)\n",
        "sns.lineplot(x=range(len(acc)), y=val_acc)\n",
        "plt.title('with_BERT_model accuracy')\n",
        "plt.legend(['acc', 'val_acc']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAjF7aatIcsH"
      },
      "source": [
        "# 50 epoch동안 test set 평균 정확도\n",
        "sum(history3.history['val_accuracy'])/50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nnh7u4RzYwu"
      },
      "source": [
        "loss = history3.history['loss']\n",
        "val_loss = history3.history['val_loss']\n",
        "\n",
        "sns.lineplot(x=range(len(acc)), y=loss)\n",
        "sns.lineplot(x=range(len(acc)), y=val_loss)\n",
        "plt.title('with_BERT_model loss')\n",
        "plt.legend(['loss', 'val_loss']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "870BXGHvL1bV"
      },
      "source": [
        "# bert_model.save(BASE_PATH+\"with_BERT_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXXYmGSZ3ELb"
      },
      "source": [
        "experts_wiki_books기반의 Bert를 Fine Tuning하여 학습하였다.\n",
        "\n",
        "학습을 진행할수록 train set에 대한 loss는 감소하고 정확도는 증가하는 모습을 보였다.\n",
        "\n",
        "하지만, 다른 모델들과 마찬가지로 test set의 loss는 꾸준히 증가하는 경향을 나타냈고,\n",
        "\n",
        "정확도는 특별한 경향성을 보이지 않고 오르락 내리락 하는 형태만 계속해서 나타냈다.\n",
        "\n",
        "이에 더 이상의 학습은 무의미하다고 판단하여 50epoch까지만 학습을 진행하였다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwV6_EDxJwLF"
      },
      "source": [
        "## RandomForest를 이용한 예측 및 결론"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KrjjAK0Um_5"
      },
      "source": [
        "### TF-IDF + RandomForest\n",
        "그렇다면 머신러닝 방법론을 적용해보면 어떨까? 하는 생각을 가지게 되었다.\n",
        "\n",
        "머신러닝에 데이터를 학습시키기 위해서는 데이터의 특성을 추출해낼 필요가 있는데,\n",
        "\n",
        "이를 위해 앞에서 사용했던 TF-IDF분류를 통해 생성한 특성을 사용하여 학습을 진행하였다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEKxGzQ6y6xD"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaY3tfOmzbrU"
      },
      "source": [
        "rfc = RandomForestClassifier(random_state=11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKd2-oCrzelL"
      },
      "source": [
        "rfc.fit(train_tfidf, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3QvLmV4zpNG"
      },
      "source": [
        "rfc.score(train_tfidf, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vYlIZAgzuLp"
      },
      "source": [
        "rfc.score(test_tfidf, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8ygNOaRVdqS"
      },
      "source": [
        "그 결과는 아무런 튜닝을 거치지 않은 기본적인 모델임에도 불구하고, \n",
        "\n",
        "train set 정확도 1.0 / test set 정확도 0.1511로 나타났다.\n",
        "\n",
        "이 결과는 앞에서 시도했던 딥러닝 방법론들과 유사한 성능으로 모델을 학습시키는데 필요한 리소스, 시간등을 고려했을때 오히려 더 나은 선택일 수 있다.\n",
        "\n",
        "하지만 결과적으로 시험해본 모든 모델은 train set에 과적합되고 test set에 대해서는 전혀 예측하지 못하는 모습을 보였다.\n",
        "\n",
        "그 이유로는 다양한것이 존재할 수 있겠지만, \n",
        "\n"
      ]
    }
  ]
}